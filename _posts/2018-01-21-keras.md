---
layout: post
title:  Keras-python
subtitle:   Deep learning/持续更新
date:   2018-01-22
author: gavin
header-img: img/post-bg-deeplearning.png
catalog:    true
tags:
    - deep learning
---

>记录一下keras-python学习过程

# KNN

### description

KNN是一种用于分类和回归的非参数统计(nonparametric)方法，非参数统计即不对样本分布做假设，直接分析样本的一类统计方法。
简单来说, KNN指训练完train set后，当给出test时，根据离test距离最近的k个train的值，确定test的值。
这里距离test的距离分为两种：L1 distance/ L2 distance。L1 distance也叫manhattan distance，计算的是两点在坐标系上的截距总和，因此L1 distance依赖coordinate system; L2 distance也叫Euclidean distance，计算的是两点的直线距离，不依赖coordinate system。

在KNN中，K被称为hyper-parameter，需要我们在训练的时候调整来达到better performance。目前比较好的方法是把数据分为train, validation, test。训练train set, 用validation set evaluate， 找到best performance的k去跑test。

KNN不适合用于vision recognition。

附上KNN demo: [KNN](http://vision.stanford.edu/teaching/cs231n-demos/knn/)

### Implementation(iris)

```python

from sklearn.datasets import load_iris  
from sklearn import neighbors  
import sklearn
iris = load_iris()
knn = neighbors.KNeighborsClassifier().fit(iris.data, iris.target)
predict = knn.predict([[0.1,0.2,0.3,0.4]])
print(predict)
print iris.target_names[predict]

```

About `sklearn.neighbor.KNeighborsClassifier()`
```python
###other example
X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3).fit(X,y)
print(neigh.predict([[1.1]]))

```
Some methods:

Name | description
---- | -----------
fit(X,y) | Fit the model using X as training data and y as target values
predict(X) | Predict the class labels for the provided data
predict_proba(X) | Return probability estimates for the test data X.

more details:[sklearn.neighbors.KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)

# Sequential Model/序贯模型

简历一个简单的Sequential model

```python
from keras.models import Sequential

model = Sequential()
```

接下来为该模型建立layers

```python
from keras.layers import Dense, Dropout
model.add(Dense(64, input_dim = 20, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation = 'sigmoid'))

```
然后compile我们的模型
```python
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
```

Generate dummy data

```python
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, 20))
y_test = np.random.randint(2, size=(100, 1))

```

训练开始

```python
model.fit(x_train, y_train,batch_size = 200,epochs = 10)
```

## 补充

### tensor 张量

```python
>>> import numpy as np
>>> import math
>>> a = np.array([[1,2,3],[4,5,6]])
>>> print(a)
[[1 2 3]
 [4 5 6]]
>>> a.shape()
(2, 3)
>>> b = np.array([1,2,3])
>>> print(b)
[1 2 3]
>>> np.shape(b)
(3,)

```
与矩阵中概念不同，shape(2,3)指第一维有2个元素，第二维有3个元素

### Definitions of parameters

Dense即为全连接层，意为层与层之间所有节点都互相相连。

keras.layers.Dense(units, input_dim, activation)

`units` output space

`input_dim` 对输入数据的维度要求，只在第一层做要求。

`activation` 激活函数

keras.layers.Dropout(rate)

`rate` 每次batch忽略结点的rate

`batch_size` 一次iteration需要跑的sample数量。

`epochs` 跑完一次全部数据

综上， epoch = nums of iterations = nusm of samples / batch_size


# MLP (Multi-Layer Perceptron) 多层感知器

除了input layer, output layer，至少有一个hidden layer ，形式上full-connected

### 使用MLP实现mnist

```python
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils

seed = 7
numpy.random.seed(seed)
data = numpy.load('mnist.npz')

X_train, y_train = data['x_train'],data['y_train']
X_test, y_test = data['x_test'], data['y_test']

num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
# for 0~255 to 0~1
X_train = X_train/255
X_test = X_test/255
# convert to 'one-hot' format
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
# create model
model = Sequential()
model.add(Dense(num_pixels, input_dim = num_pixels, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
# compile model
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)
scores = model.evaluate(X_test, y_test, verbose = 0)

print("MLP Error: %.2f%%" % (100-scores[1]*100))
```

*Why one-hot?*

Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. For e.g. red[1,0,0], green[0,1,0], blue[0,0,1].



