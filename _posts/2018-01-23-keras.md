---
layout: post
title:  Keras
subtitle:   Deep learning/Updating
date:   2018-01-23
author: gavin
header-img: img/post-bg-deeplearning.png
catalog:    true
tags:
    - deep learning
---

>Recording the process of studying keras

# Sequential Model

Buliding a simple Sequential Model

```python
from keras.models import Sequential

model = Sequential()
```

Add layers for the model

```python
from keras.layers import Dense, Dropout
model.add(Dense(64, input_dim = 20, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation = 'sigmoid'))

```
Compiling our model.

```python
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
```

Generate dummy data

```python
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, 20))
y_test = np.random.randint(2, size=(100, 1))

```

Start training

```python
model.fit(x_train, y_train,batch_size = 200,epochs = 10)
```

## PS

### tensor 

```python
>>> import numpy as np
>>> import math
>>> a = np.array([[1,2,3],[4,5,6]])
>>> print(a)
[[1 2 3]
 [4 5 6]]
>>> a.shape()
(2, 3)
>>> b = np.array([1,2,3])
>>> print(b)
[1 2 3]
>>> np.shape(b)
(3,)

```
与矩阵中概念不同，shape(2,3)指第一维有2个元素，第二维有3个元素

### Definitions of parameters

Dense is full connected layer.

`keras.layers.Dense(units, input_dim, activation)`<br>
`units` output space<br>
`input_dim` the requirement of the input_dim, only requiring it at the begining<br>
`activation`  activation function<br>

`keras.layers.Dropout(rate)`<br>
`rate` the rate of nodes which are ignored during each batch.<br>
`batch_size` the number of sample in one iteration.<br>
`epochs` go through all the samples.

From above， epoch = nums of iterations = nusm of samples / batch_size


# MLP (Multi-Layer Perceptron) 

除了input layer, output layer，至少有一个hidden layer ，形式上full-connected

### Mnist using MLP

```python
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils

seed = 7
numpy.random.seed(seed)
data = numpy.load('mnist.npz')

X_train, y_train = data['x_train'],data['y_train']
X_test, y_test = data['x_test'], data['y_test']

num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
# for 0~255 to 0~1
X_train = X_train/255
X_test = X_test/255
# convert to 'one-hot' format
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
# create model
model = Sequential()
model.add(Dense(num_pixels, input_dim = num_pixels, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
# compile model
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)
scores = model.evaluate(X_test, y_test, verbose = 0)

print("MLP Error: %.2f%%" % (100-scores[1]*100))
```

*Why one-hot?*

Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. For e.g. red[1,0,0], green[0,1,0], blue[0,0,1].

# CNN (Convolutional Nerual Network)

### Mnist using CNN

```python

import numpy
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('th')

seed = 7
numpy.random.seed(seed)
data = numpy.load('mnist.npz')
X_train, y_train = data['x_train'],data['y_train']
X_test, y_test = data['x_test'], data['y_test']
X_train = X_train.reshape(X_train.shape[0], 1,28,28).astype('float32')
X_test = X_test.reshape(X_test.shape[0], 1,28,28).astype('float32')
# normalize inputs from 0-255 to 0-1
X_train = X_train/255
X_test = X_test/255
# convert to 'one-hot' format
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
# create model
model = Sequential()
model.add(Conv2D(32,(5,5), input_shape = (1,28,28), activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
# compile model
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
# fit the model
model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)
# evaluation of the model
scores = model.evaluate(X_test, y_test, verbose = 0)
print('CNN Error: %.2f%%' % (100 - scores[1]*100))

```
`K.set_image_dim_ordering('th')` <br>
当图片维序类型为th时，输入数据格式为[samples][channels][rows][cols];<br>
当图片维序类型为tf时，输入数据格式为[samples][rows][cols][channels];<br>
channels代表图片的RGB通道，彩色图片为3，灰度图片为1

`keras.layers.Conv2D(nums of filters, kernel_size, input_shape)`<br>
`nums of filters` actually it's nums of neurons, since each neuron perform a different convolution on the input to the layer (more precisely, the neurons' input weights form convolution kernels). A feature map is the result of applying a filter (thus, you have as many feature maps as filters), and its size is a result of window/kernel size of your filter and stride.<br>

